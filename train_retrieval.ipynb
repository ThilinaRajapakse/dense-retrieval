{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Retrieval with Simple Transformers - Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 15:14:35.128017: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import json\n",
    "\n",
    "from simpletransformers.retrieval import RetrievalModel, RetrievalArgs\n",
    "import wandb\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be downloaded from https://github.com/facebookresearch/DPR\n",
    "\n",
    "```bash\n",
    "# Downloads the dataset from https://github.com/facebookresearch/DPR\n",
    "python download_data.py --resource data.retriever.nq-train\n",
    "python download_data.py --resource data.retriever.nq-dev\n",
    "\n",
    "# Move things around and clean up\n",
    "mv downloads/data .\n",
    "mv data/retriever/* data\n",
    "rm -r data/retriever\n",
    "rm -r downloads\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = \"data/nq-train.json\"\n",
    "eval_data = \"data/nq-dev.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_data, \"r\") as f:\n",
    "    train = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DPR format\n",
    "\n",
    "A JSON file where each entry is a dictionary must contain the two keys:\n",
    "- `question`\n",
    "- `postive_ctxs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dataset', 'question', 'answers', 'positive_ctxs', 'negative_ctxs', 'hard_negative_ctxs'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `positive_ctxs` is a list of relevant documents where each entry must contain the two keys:\n",
    "- `title`\n",
    "- `text`\n",
    "\n",
    "While this list may contain multiple relevant documents, we only use the first (most) relevant document during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'text', 'score', 'title_score', 'passage_id'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][\"positive_ctxs\"][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, Simple Transformers looks for a query and the relevant doc/passage for that query. The relevant passage may also contain an optional `title` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: big little lies season 2 how many episodes\n",
      "\n",
      "Relevant passage title: Big Little Lies (TV series)\n",
      "\n",
      "Relevant passage title: series garnered several accolades. It received 16 Emmy Award nominations and won eight, including Outstanding Limited Series and acting awards for Kidman, Skarsgård, and Dern. The trio also won Golden Globe Awards in addition to a Golden Globe Award for Best Miniseries or Television Film win for the series. Kidman and Skarsgård also received Screen Actors Guild Awards for their performances. Despite originally being billed as a miniseries, HBO renewed the series for a second season. Production on the second season began in March 2018 and is set to premiere in 2019. All seven episodes are being written by Kelley\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query: {train[0]['question']}\")\n",
    "print()\n",
    "print(f\"Relevant passage title: {train[0]['positive_ctxs'][0]['title']}\")\n",
    "print()\n",
    "print(f\"Relevant passage title: {train[0]['positive_ctxs'][0]['text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended data format for custom datasets\n",
    "\n",
    "### TSV file\n",
    "\n",
    "The recommended data format for custom datasets is a simple TSV file with the following 3 columns:\n",
    "- `query_text`\n",
    "- `gold_passage`\n",
    "- `title`\n",
    "\n",
    "Alternatively, a Pandas Dataframe with the same columns may be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Model\n",
    "\n",
    "The retrieval model is a dual encoder consisting of two BERT encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"custom\"\n",
    "model_name = None\n",
    "context_name = \"bert-base-uncased\"\n",
    "question_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = RetrievalArgs()\n",
    "\n",
    "# Training parameters\n",
    "model_args.num_train_epochs = 40\n",
    "model_args.train_batch_size = 40\n",
    "model_args.learning_rate = 1e-5\n",
    "model_args.max_seq_length = 256\n",
    "\n",
    "# Evaluation parameters\n",
    "model_args.retrieve_n_docs = 100\n",
    "model_args.eval_batch_size = 100\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_verbose = True\n",
    "# model_args.evaluate_during_training_steps = 200\n",
    "\n",
    "# Model tracking\n",
    "model_args.wandb_project = \"Dense retrieval with Simple Transformers\"\n",
    "model_args.save_model_every_epoch = False\n",
    "model_args.save_eval_checkpoints = False\n",
    "model_args.save_steps = -1\n",
    "model_args.save_best_model = True\n",
    "model_args.overwrite_output_dir = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = RetrievalModel(\n",
    "    model_type=model_type,\n",
    "    model_name=model_name,\n",
    "    context_encoder_name=context_name,\n",
    "    query_encoder_name=question_name,\n",
    "    args=model_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "To train the model, we simply call `train_model` and pass the path to the training data file, along with the validation data file (required for evaluating during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.load:Using the latest cached version of the module from /deep_learning/.cache/huggingface/modules/datasets_modules/datasets/retrieval_dataset_loading_script/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3 (last modified on Tue Jan 18 16:11:08 2022) since it couldn't be found locally at retrieval_dataset_loading_script., or remotely on the Hugging Face Hub.\n",
      "WARNING:datasets.builder:Using custom data configuration default-e40ee4a6467d4bb8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset retrieval_dataset_loading_script/default (download: Unknown size, generated: 38.37 MiB, post-processed: Unknown size, total: 38.37 MiB) to /deep_learning/.cache/huggingface/datasets/retrieval_dataset_loading_script/default-e40ee4a6467d4bb8/0.0.0/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/58880 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset retrieval_dataset_loading_script downloaded and prepared to /deep_learning/.cache/huggingface/datasets/retrieval_dataset_loading_script/default-e40ee4a6467d4bb8/0.0.0/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11003431657c4edf9242141cab9db9f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3a2d1432e841998a9c49cd5c825bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thilina/miniconda3/envs/st/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "INFO:simpletransformers.retrieval.retrieval_model: Training started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7cb41ec4974d11bf86c58b52771fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthilina\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "2022-08-18 12:21:57.985474: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.32<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">glamorous-dust-21</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/thilina/Training%20DPR%20Tests\" target=\"_blank\">https://wandb.ai/thilina/Training%20DPR%20Tests</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/thilina/Training%20DPR%20Tests/runs/3uazt1x0\" target=\"_blank\">https://wandb.ai/thilina/Training%20DPR%20Tests/runs/3uazt1x0</a><br/>\n",
       "                Run data is saved locally in <code>/deep_learning/WandB Seminar/wandb/run-20220818_122155-3uazt1x0</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf100d5b079491899fc531e9e40af41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 5:   0%|          | 0/1472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thilina/Projects/simple-transformers/simpletransformers/retrieval/retrieval_model.py:1737: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  (max_idxs == torch.tensor(labels)).sum().cpu().detach().numpy().item()\n",
      "/home/thilina/miniconda3/envs/st/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "INFO:simpletransformers.retrieval.retrieval_utils:Loading evaluation passages to a Huggingface Dataset\n",
      "WARNING:datasets.load:Using the latest cached version of the module from /deep_learning/.cache/huggingface/modules/datasets_modules/datasets/retrieval_dataset_loading_script/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3 (last modified on Tue Jan 18 16:11:08 2022) since it couldn't be found locally at retrieval_dataset_loading_script., or remotely on the Hugging Face Hub.\n",
      "WARNING:datasets.builder:Using custom data configuration default-6677aded6fea238d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset retrieval_dataset_loading_script/default (download: Unknown size, generated: 4.24 MiB, post-processed: Unknown size, total: 4.24 MiB) to /deep_learning/.cache/huggingface/datasets/retrieval_dataset_loading_script/default-6677aded6fea238d/0.0.0/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/6515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset retrieval_dataset_loading_script downloaded and prepared to /deep_learning/.cache/huggingface/datasets/retrieval_dataset_loading_script/default-6677aded6fea238d/0.0.0/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1149f664030947d896dfd510fb63acba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_utils:Loading evaluation passages to a Huggingface Dataset completed.\n",
      "INFO:simpletransformers.retrieval.retrieval_utils:Generating embeddings for evaluation passages\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d94d14597dd4616bf7a9a7734b06d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_utils:Generating embeddings for evaluation passages completed.\n",
      "INFO:simpletransformers.retrieval.retrieval_utils:Adding FAISS index to evaluation passages\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08efc363619d4cb08258cd40ac08da6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_utils:Adding FAISS index to evaluation passages completed.\n",
      "WARNING:datasets.load:Using the latest cached version of the module from /deep_learning/.cache/huggingface/modules/datasets_modules/datasets/retrieval_dataset_loading_script/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3 (last modified on Tue Jan 18 16:11:08 2022) since it couldn't be found locally at retrieval_dataset_loading_script., or remotely on the Hugging Face Hub.\n",
      "WARNING:datasets.builder:Using custom data configuration default-6677aded6fea238d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset retrieval_dataset_loading_script/default (download: Unknown size, generated: 4.24 MiB, post-processed: Unknown size, total: 4.24 MiB) to /deep_learning/.cache/huggingface/datasets/retrieval_dataset_loading_script/default-6677aded6fea238d/0.0.0/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/6515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset retrieval_dataset_loading_script downloaded and prepared to /deep_learning/.cache/huggingface/datasets/retrieval_dataset_loading_script/default-6677aded6fea238d/0.0.0/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457e284319704d8f9c1a656c0e3bc877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f1c30247544915acc894903a433f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dec967d7aaf42c1bc1158adefcffb10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving docs:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_model: Initializing WandB run for evaluation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3uazt1x0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2765751<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.05MB of 0.05MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/deep_learning/WandB Seminar/wandb/run-20220818_122155-3uazt1x0/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/deep_learning/WandB Seminar/wandb/run-20220818_122155-3uazt1x0/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Training loss</td><td>0.27994</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>global_step</td><td>1450</td></tr><tr><td>_runtime</td><td>747</td></tr><tr><td>_timestamp</td><td>1660818863</td></tr><tr><td>_step</td><td>28</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Training loss</td><td>█▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>▁▂▃▄▅▅▆▇█████████▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██</td></tr><tr><td>_runtime</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">glamorous-dust-21</strong>: <a href=\"https://wandb.ai/thilina/Training%20DPR%20Tests/runs/3uazt1x0\" target=\"_blank\">https://wandb.ai/thilina/Training%20DPR%20Tests/runs/3uazt1x0</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:3uazt1x0). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "2022-08-18 12:38:01.996084: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.32<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">feasible-resonance-22</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/thilina/Training%20DPR%20Tests\" target=\"_blank\">https://wandb.ai/thilina/Training%20DPR%20Tests</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/thilina/Training%20DPR%20Tests/runs/15bdiivr\" target=\"_blank\">https://wandb.ai/thilina/Training%20DPR%20Tests/runs/15bdiivr</a><br/>\n",
       "                Run data is saved locally in <code>/deep_learning/WandB Seminar/wandb/run-20220818_123756-15bdiivr</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_model:{'eval_loss': 16.66825433210893, 'mrr@1': 0.5493476592478895, 'mrr@2': 0.6128933231005372, 'mrr@3': 0.6332054233819392, 'mrr@5': 0.6478255308262983, 'mrr@10': 0.6558959787547661, 'top_1_accuracy': 0.5493476592478895, 'top_2_accuracy': 0.6764389869531849, 'top_3_accuracy': 0.7373752877973906, 'top_5_accuracy': 0.8006139677666922, 'top_10_accuracy': 0.8597083653108212}\n",
      "INFO:simpletransformers.retrieval.retrieval_model:Saving model into outputs/best_model\n",
      "/home/thilina/miniconda3/envs/st/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63bb25b4e158456e98312b4c91e5f2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 5:   0%|          | 0/1472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_utils:Loading evaluation passages to a Huggingface Dataset\n",
      "WARNING:datasets.load:Using the latest cached version of the module from /deep_learning/.cache/huggingface/modules/datasets_modules/datasets/retrieval_dataset_loading_script/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3 (last modified on Tue Jan 18 16:11:08 2022) since it couldn't be found locally at retrieval_dataset_loading_script., or remotely on the Hugging Face Hub.\n",
      "WARNING:datasets.builder:Using custom data configuration default-6677aded6fea238d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset retrieval_dataset_loading_script/default (download: Unknown size, generated: 4.24 MiB, post-processed: Unknown size, total: 4.24 MiB) to /deep_learning/.cache/huggingface/datasets/retrieval_dataset_loading_script/default-6677aded6fea238d/0.0.0/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/6515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset retrieval_dataset_loading_script downloaded and prepared to /deep_learning/.cache/huggingface/datasets/retrieval_dataset_loading_script/default-6677aded6fea238d/0.0.0/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1d4f01f6e44476b1ec2b55709d5f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_utils:Loading evaluation passages to a Huggingface Dataset completed.\n",
      "INFO:simpletransformers.retrieval.retrieval_utils:Generating embeddings for evaluation passages\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44f8131c912404cb67644123d63fe7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_utils:Generating embeddings for evaluation passages completed.\n",
      "INFO:simpletransformers.retrieval.retrieval_utils:Adding FAISS index to evaluation passages\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17077e39e01f48d7bf1e4fa16dca8942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_utils:Adding FAISS index to evaluation passages completed.\n",
      "WARNING:datasets.load:Using the latest cached version of the module from /deep_learning/.cache/huggingface/modules/datasets_modules/datasets/retrieval_dataset_loading_script/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3 (last modified on Tue Jan 18 16:11:08 2022) since it couldn't be found locally at retrieval_dataset_loading_script., or remotely on the Hugging Face Hub.\n",
      "WARNING:datasets.builder:Using custom data configuration default-6677aded6fea238d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset retrieval_dataset_loading_script/default (download: Unknown size, generated: 4.24 MiB, post-processed: Unknown size, total: 4.24 MiB) to /deep_learning/.cache/huggingface/datasets/retrieval_dataset_loading_script/default-6677aded6fea238d/0.0.0/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/6515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset retrieval_dataset_loading_script downloaded and prepared to /deep_learning/.cache/huggingface/datasets/retrieval_dataset_loading_script/default-6677aded6fea238d/0.0.0/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b754d440e748b7b927880005408a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f59642cbd84f11a0ad447c94ccbd29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5089d699a748bb9e627d23c3170450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving docs:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_model: Initializing WandB run for evaluation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:15bdiivr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2983431<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/deep_learning/WandB Seminar/wandb/run-20220818_123756-15bdiivr/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/deep_learning/WandB Seminar/wandb/run-20220818_123756-15bdiivr/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval_loss</td><td>16.66825</td></tr><tr><td>mrr@1</td><td>0.54935</td></tr><tr><td>mrr@2</td><td>0.61289</td></tr><tr><td>mrr@3</td><td>0.63321</td></tr><tr><td>mrr@5</td><td>0.64783</td></tr><tr><td>mrr@10</td><td>0.6559</td></tr><tr><td>top_1_accuracy</td><td>0.54935</td></tr><tr><td>top_2_accuracy</td><td>0.67644</td></tr><tr><td>top_3_accuracy</td><td>0.73738</td></tr><tr><td>top_5_accuracy</td><td>0.80061</td></tr><tr><td>top_10_accuracy</td><td>0.85971</td></tr><tr><td>_runtime</td><td>280</td></tr><tr><td>_timestamp</td><td>1660819360</td></tr><tr><td>_step</td><td>12</td></tr><tr><td>global_step</td><td>2000</td></tr><tr><td>train_loss</td><td>0.14172</td></tr><tr><td>Training loss</td><td>0.05084</td></tr><tr><td>lr</td><td>1e-05</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval_loss</td><td>▁▁</td></tr><tr><td>mrr@1</td><td>▁▁</td></tr><tr><td>mrr@2</td><td>▁▁</td></tr><tr><td>mrr@3</td><td>▁▁</td></tr><tr><td>mrr@5</td><td>▁▁</td></tr><tr><td>mrr@10</td><td>▁▁</td></tr><tr><td>top_1_accuracy</td><td>▁▁</td></tr><tr><td>top_2_accuracy</td><td>▁▁</td></tr><tr><td>top_3_accuracy</td><td>▁▁</td></tr><tr><td>top_5_accuracy</td><td>▁▁</td></tr><tr><td>top_10_accuracy</td><td>▁▁</td></tr><tr><td>_runtime</td><td>▁▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>_timestamp</td><td>▁▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>_step</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█</td></tr><tr><td>global_step</td><td>▁▁▂▃▃▄▅▅▆▇▇█</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>Training loss</td><td>▄▃▁▄▄▁█▄▂▂▁</td></tr><tr><td>lr</td><td>█▇▇▆▅▅▄▃▂▂▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">feasible-resonance-22</strong>: <a href=\"https://wandb.ai/thilina/Training%20DPR%20Tests/runs/15bdiivr\" target=\"_blank\">https://wandb.ai/thilina/Training%20DPR%20Tests/runs/15bdiivr</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:15bdiivr). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "2022-08-18 12:46:01.875104: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.32<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">wandering-sponge-23</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/thilina/Training%20DPR%20Tests\" target=\"_blank\">https://wandb.ai/thilina/Training%20DPR%20Tests</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/thilina/Training%20DPR%20Tests/runs/1gz5js5t\" target=\"_blank\">https://wandb.ai/thilina/Training%20DPR%20Tests/runs/1gz5js5t</a><br/>\n",
       "                Run data is saved locally in <code>/deep_learning/WandB Seminar/wandb/run-20220818_124556-1gz5js5t</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_model:{'eval_loss': 18.92426655509255, 'mrr@1': 0.5857252494244052, 'mrr@2': 0.6488104374520338, 'mrr@3': 0.6676899462778204, 'mrr@5': 0.6815272448196469, 'mrr@10': 0.6886760101840685, 'top_1_accuracy': 0.5857252494244052, 'top_2_accuracy': 0.7118956254796623, 'top_3_accuracy': 0.7685341519570222, 'top_5_accuracy': 0.8283960092095165, 'top_10_accuracy': 0.8805832693783576}\n",
      "INFO:tensorboardX.summary:Summary name eval_mrr@1 is illegal; using eval_mrr_1 instead.\n",
      "INFO:tensorboardX.summary:Summary name eval_mrr@2 is illegal; using eval_mrr_2 instead.\n",
      "INFO:tensorboardX.summary:Summary name eval_mrr@3 is illegal; using eval_mrr_3 instead.\n",
      "INFO:tensorboardX.summary:Summary name eval_mrr@5 is illegal; using eval_mrr_5 instead.\n",
      "INFO:tensorboardX.summary:Summary name eval_mrr@10 is illegal; using eval_mrr_10 instead.\n",
      "INFO:simpletransformers.retrieval.retrieval_utils:Loading evaluation passages to a Huggingface Dataset\n",
      "WARNING:datasets.load:Using the latest cached version of the module from /deep_learning/.cache/huggingface/modules/datasets_modules/datasets/retrieval_dataset_loading_script/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3 (last modified on Tue Jan 18 16:11:08 2022) since it couldn't be found locally at retrieval_dataset_loading_script., or remotely on the Hugging Face Hub.\n",
      "WARNING:datasets.builder:Using custom data configuration default-6677aded6fea238d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset retrieval_dataset_loading_script/default (download: Unknown size, generated: 4.24 MiB, post-processed: Unknown size, total: 4.24 MiB) to /deep_learning/.cache/huggingface/datasets/retrieval_dataset_loading_script/default-6677aded6fea238d/0.0.0/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/6515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset retrieval_dataset_loading_script downloaded and prepared to /deep_learning/.cache/huggingface/datasets/retrieval_dataset_loading_script/default-6677aded6fea238d/0.0.0/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f48d8efceb244438fb09839258d60c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_utils:Loading evaluation passages to a Huggingface Dataset completed.\n",
      "INFO:simpletransformers.retrieval.retrieval_utils:Generating embeddings for evaluation passages\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f9e9aac22e4073bcdcfaf0d864c797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_utils:Generating embeddings for evaluation passages completed.\n",
      "INFO:simpletransformers.retrieval.retrieval_utils:Adding FAISS index to evaluation passages\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f17371888c4018827329bb5a0464c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_utils:Adding FAISS index to evaluation passages completed.\n",
      "WARNING:datasets.load:Using the latest cached version of the module from /deep_learning/.cache/huggingface/modules/datasets_modules/datasets/retrieval_dataset_loading_script/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3 (last modified on Tue Jan 18 16:11:08 2022) since it couldn't be found locally at retrieval_dataset_loading_script., or remotely on the Hugging Face Hub.\n",
      "WARNING:datasets.builder:Using custom data configuration default-6677aded6fea238d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset retrieval_dataset_loading_script/default (download: Unknown size, generated: 4.24 MiB, post-processed: Unknown size, total: 4.24 MiB) to /deep_learning/.cache/huggingface/datasets/retrieval_dataset_loading_script/default-6677aded6fea238d/0.0.0/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/6515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset retrieval_dataset_loading_script downloaded and prepared to /deep_learning/.cache/huggingface/datasets/retrieval_dataset_loading_script/default-6677aded6fea238d/0.0.0/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445e6cb21aac4b399545393db48975dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6103580d163f426199930c9fdaa5ff37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da920c1def70448a9a37bdd8fac119d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving docs:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_model: Initializing WandB run for evaluation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1gz5js5t) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3046593<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/deep_learning/WandB Seminar/wandb/run-20220818_124556-1gz5js5t/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/deep_learning/WandB Seminar/wandb/run-20220818_124556-1gz5js5t/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval_loss</td><td>18.92427</td></tr><tr><td>mrr@1</td><td>0.58573</td></tr><tr><td>mrr@2</td><td>0.64881</td></tr><tr><td>mrr@3</td><td>0.66769</td></tr><tr><td>mrr@5</td><td>0.68153</td></tr><tr><td>mrr@10</td><td>0.68868</td></tr><tr><td>top_1_accuracy</td><td>0.58573</td></tr><tr><td>top_2_accuracy</td><td>0.7119</td></tr><tr><td>top_3_accuracy</td><td>0.76853</td></tr><tr><td>top_5_accuracy</td><td>0.8284</td></tr><tr><td>top_10_accuracy</td><td>0.88058</td></tr><tr><td>_runtime</td><td>455</td></tr><tr><td>_timestamp</td><td>1660820015</td></tr><tr><td>_step</td><td>19</td></tr><tr><td>global_step</td><td>2900</td></tr><tr><td>train_loss</td><td>0.05084</td></tr><tr><td>Training loss</td><td>0.14423</td></tr><tr><td>lr</td><td>1e-05</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval_loss</td><td>▁▁</td></tr><tr><td>mrr@1</td><td>▁▁</td></tr><tr><td>mrr@2</td><td>▁▁</td></tr><tr><td>mrr@3</td><td>▁▁</td></tr><tr><td>mrr@5</td><td>▁▁</td></tr><tr><td>mrr@10</td><td>▁▁</td></tr><tr><td>top_1_accuracy</td><td>▁▁</td></tr><tr><td>top_2_accuracy</td><td>▁▁</td></tr><tr><td>top_3_accuracy</td><td>▁▁</td></tr><tr><td>top_5_accuracy</td><td>▁▁</td></tr><tr><td>top_10_accuracy</td><td>▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>_step</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>global_step</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>Training loss</td><td>▇▃▃▁▃▂▂▁▅▂▃▅▃▂█▂▂▄</td></tr><tr><td>lr</td><td>██▇▇▆▆▆▅▅▄▄▃▃▃▂▂▁▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">wandering-sponge-23</strong>: <a href=\"https://wandb.ai/thilina/Training%20DPR%20Tests/runs/1gz5js5t\" target=\"_blank\">https://wandb.ai/thilina/Training%20DPR%20Tests/runs/1gz5js5t</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:1gz5js5t). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "2022-08-18 12:57:15.515633: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.32<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">curious-oath-24</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/thilina/Training%20DPR%20Tests\" target=\"_blank\">https://wandb.ai/thilina/Training%20DPR%20Tests</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/thilina/Training%20DPR%20Tests/runs/65mehj25\" target=\"_blank\">https://wandb.ai/thilina/Training%20DPR%20Tests/runs/65mehj25</a><br/>\n",
       "                Run data is saved locally in <code>/deep_learning/WandB Seminar/wandb/run-20220818_125710-65mehj25</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_model:{'eval_loss': 20.71651282454982, 'mrr@1': 0.642056792018419, 'mrr@2': 0.7044512663085188, 'mrr@3': 0.7226144794064979, 'mrr@5': 0.7344333589153237, 'mrr@10': 0.7401528219371658, 'top_1_accuracy': 0.642056792018419, 'top_2_accuracy': 0.7668457405986185, 'top_3_accuracy': 0.8213353798925557, 'top_5_accuracy': 0.8726016884113584, 'top_10_accuracy': 0.9146584804297775}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e078906f3094598bce2daec5113c641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 5:   0%|          | 0/1472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_utils:Loading evaluation passages to a Huggingface Dataset\n",
      "WARNING:datasets.load:Using the latest cached version of the module from /deep_learning/.cache/huggingface/modules/datasets_modules/datasets/retrieval_dataset_loading_script/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3 (last modified on Tue Jan 18 16:11:08 2022) since it couldn't be found locally at retrieval_dataset_loading_script., or remotely on the Hugging Face Hub.\n",
      "WARNING:datasets.builder:Using custom data configuration default-6677aded6fea238d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset retrieval_dataset_loading_script/default (download: Unknown size, generated: 4.24 MiB, post-processed: Unknown size, total: 4.24 MiB) to /deep_learning/.cache/huggingface/datasets/retrieval_dataset_loading_script/default-6677aded6fea238d/0.0.0/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/6515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset retrieval_dataset_loading_script downloaded and prepared to /deep_learning/.cache/huggingface/datasets/retrieval_dataset_loading_script/default-6677aded6fea238d/0.0.0/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae35be474044211a74dbc4546291bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_utils:Loading evaluation passages to a Huggingface Dataset completed.\n",
      "INFO:simpletransformers.retrieval.retrieval_utils:Generating embeddings for evaluation passages\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62fa2db2c9c4dd8b7e63661a09f232b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_utils:Generating embeddings for evaluation passages completed.\n",
      "INFO:simpletransformers.retrieval.retrieval_utils:Adding FAISS index to evaluation passages\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def913014f3844749bde7a8043690299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_utils:Adding FAISS index to evaluation passages completed.\n",
      "WARNING:datasets.load:Using the latest cached version of the module from /deep_learning/.cache/huggingface/modules/datasets_modules/datasets/retrieval_dataset_loading_script/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3 (last modified on Tue Jan 18 16:11:08 2022) since it couldn't be found locally at retrieval_dataset_loading_script., or remotely on the Hugging Face Hub.\n",
      "WARNING:datasets.builder:Using custom data configuration default-6677aded6fea238d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset retrieval_dataset_loading_script/default (download: Unknown size, generated: 4.24 MiB, post-processed: Unknown size, total: 4.24 MiB) to /deep_learning/.cache/huggingface/datasets/retrieval_dataset_loading_script/default-6677aded6fea238d/0.0.0/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/6515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset retrieval_dataset_loading_script downloaded and prepared to /deep_learning/.cache/huggingface/datasets/retrieval_dataset_loading_script/default-6677aded6fea238d/0.0.0/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deee464ec5184914afb1eb9467f2bb67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4633f1adbb3b4d22867a1e7a28318db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6a29f7f1d74a53bc6793e9f8e70797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving docs:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_model: Initializing WandB run for evaluation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:65mehj25) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3117738<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/deep_learning/WandB Seminar/wandb/run-20220818_125710-65mehj25/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/deep_learning/WandB Seminar/wandb/run-20220818_125710-65mehj25/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval_loss</td><td>20.71651</td></tr><tr><td>mrr@1</td><td>0.64206</td></tr><tr><td>mrr@2</td><td>0.70445</td></tr><tr><td>mrr@3</td><td>0.72261</td></tr><tr><td>mrr@5</td><td>0.73443</td></tr><tr><td>mrr@10</td><td>0.74015</td></tr><tr><td>top_1_accuracy</td><td>0.64206</td></tr><tr><td>top_2_accuracy</td><td>0.76685</td></tr><tr><td>top_3_accuracy</td><td>0.82134</td></tr><tr><td>top_5_accuracy</td><td>0.8726</td></tr><tr><td>top_10_accuracy</td><td>0.91466</td></tr><tr><td>_runtime</td><td>531</td></tr><tr><td>_timestamp</td><td>1660820765</td></tr><tr><td>_step</td><td>23</td></tr><tr><td>global_step</td><td>4000</td></tr><tr><td>train_loss</td><td>0.07022</td></tr><tr><td>Training loss</td><td>0.00417</td></tr><tr><td>lr</td><td>0.0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval_loss</td><td>▁▁</td></tr><tr><td>mrr@1</td><td>▁▁</td></tr><tr><td>mrr@2</td><td>▁▁</td></tr><tr><td>mrr@3</td><td>▁▁</td></tr><tr><td>mrr@5</td><td>▁▁</td></tr><tr><td>mrr@10</td><td>▁▁</td></tr><tr><td>top_1_accuracy</td><td>▁▁</td></tr><tr><td>top_2_accuracy</td><td>▁▁</td></tr><tr><td>top_3_accuracy</td><td>▁▁</td></tr><tr><td>top_5_accuracy</td><td>▁▁</td></tr><tr><td>top_10_accuracy</td><td>▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>_step</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>Training loss</td><td>▃▄▁▁▃▂▂▂▂▁▇▆▂▂▄▃▄█▁▁▁▁</td></tr><tr><td>lr</td><td>██▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▁▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">curious-oath-24</strong>: <a href=\"https://wandb.ai/thilina/Training%20DPR%20Tests/runs/65mehj25\" target=\"_blank\">https://wandb.ai/thilina/Training%20DPR%20Tests/runs/65mehj25</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:65mehj25). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "2022-08-18 13:09:20.713398: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.32<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">devout-feather-25</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/thilina/Training%20DPR%20Tests\" target=\"_blank\">https://wandb.ai/thilina/Training%20DPR%20Tests</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/thilina/Training%20DPR%20Tests/runs/2iav62og\" target=\"_blank\">https://wandb.ai/thilina/Training%20DPR%20Tests/runs/2iav62og</a><br/>\n",
       "                Run data is saved locally in <code>/deep_learning/WandB Seminar/wandb/run-20220818_130915-2iav62og</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_model:{'eval_loss': 23.59813054402669, 'mrr@1': 0.6535686876438987, 'mrr@2': 0.7157329240214889, 'mrr@3': 0.7334868252750063, 'mrr@5': 0.7447608083908929, 'mrr@10': 0.750366797012998, 'top_1_accuracy': 0.6535686876438987, 'top_2_accuracy': 0.7778971603990791, 'top_3_accuracy': 0.8311588641596316, 'top_5_accuracy': 0.8796623177283193, 'top_10_accuracy': 0.9201841903300076}\n",
      "INFO:tensorboardX.summary:Summary name eval_mrr@1 is illegal; using eval_mrr_1 instead.\n",
      "INFO:tensorboardX.summary:Summary name eval_mrr@2 is illegal; using eval_mrr_2 instead.\n",
      "INFO:tensorboardX.summary:Summary name eval_mrr@3 is illegal; using eval_mrr_3 instead.\n",
      "INFO:tensorboardX.summary:Summary name eval_mrr@5 is illegal; using eval_mrr_5 instead.\n",
      "INFO:tensorboardX.summary:Summary name eval_mrr@10 is illegal; using eval_mrr_10 instead.\n",
      "INFO:simpletransformers.retrieval.retrieval_utils:Loading evaluation passages to a Huggingface Dataset\n",
      "WARNING:datasets.load:Using the latest cached version of the module from /deep_learning/.cache/huggingface/modules/datasets_modules/datasets/retrieval_dataset_loading_script/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3 (last modified on Tue Jan 18 16:11:08 2022) since it couldn't be found locally at retrieval_dataset_loading_script., or remotely on the Hugging Face Hub.\n",
      "WARNING:datasets.builder:Using custom data configuration default-6677aded6fea238d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset retrieval_dataset_loading_script/default (download: Unknown size, generated: 4.24 MiB, post-processed: Unknown size, total: 4.24 MiB) to /deep_learning/.cache/huggingface/datasets/retrieval_dataset_loading_script/default-6677aded6fea238d/0.0.0/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/6515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset retrieval_dataset_loading_script downloaded and prepared to /deep_learning/.cache/huggingface/datasets/retrieval_dataset_loading_script/default-6677aded6fea238d/0.0.0/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33f37b0c468429b80bd47d4fc02c372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_utils:Loading evaluation passages to a Huggingface Dataset completed.\n",
      "INFO:simpletransformers.retrieval.retrieval_utils:Generating embeddings for evaluation passages\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93990673b30c40f4b2b191fe9b4f7a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_utils:Generating embeddings for evaluation passages completed.\n",
      "INFO:simpletransformers.retrieval.retrieval_utils:Adding FAISS index to evaluation passages\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d96e7b72f6342a493c2ce24d8c19cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_utils:Adding FAISS index to evaluation passages completed.\n",
      "WARNING:datasets.load:Using the latest cached version of the module from /deep_learning/.cache/huggingface/modules/datasets_modules/datasets/retrieval_dataset_loading_script/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3 (last modified on Tue Jan 18 16:11:08 2022) since it couldn't be found locally at retrieval_dataset_loading_script., or remotely on the Hugging Face Hub.\n",
      "WARNING:datasets.builder:Using custom data configuration default-6677aded6fea238d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset retrieval_dataset_loading_script/default (download: Unknown size, generated: 4.24 MiB, post-processed: Unknown size, total: 4.24 MiB) to /deep_learning/.cache/huggingface/datasets/retrieval_dataset_loading_script/default-6677aded6fea238d/0.0.0/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/6515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset retrieval_dataset_loading_script downloaded and prepared to /deep_learning/.cache/huggingface/datasets/retrieval_dataset_loading_script/default-6677aded6fea238d/0.0.0/f0c8460ab8d4db814fa43c3f14f6e1ad1e59a0b97758c518dafbf7dcd9e00fc3. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8517a51dec439491c3d2a42b0cfb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9fcd1afead49e7a6b4d495e6dbb678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b3990b0f4a48458a5adbcc12289c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving docs:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_model: Initializing WandB run for evaluation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2iav62og) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3174905<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/deep_learning/WandB Seminar/wandb/run-20220818_130915-2iav62og/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/deep_learning/WandB Seminar/wandb/run-20220818_130915-2iav62og/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval_loss</td><td>23.59813</td></tr><tr><td>mrr@1</td><td>0.65357</td></tr><tr><td>mrr@2</td><td>0.71573</td></tr><tr><td>mrr@3</td><td>0.73349</td></tr><tr><td>mrr@5</td><td>0.74476</td></tr><tr><td>mrr@10</td><td>0.75037</td></tr><tr><td>top_1_accuracy</td><td>0.65357</td></tr><tr><td>top_2_accuracy</td><td>0.7779</td></tr><tr><td>top_3_accuracy</td><td>0.83116</td></tr><tr><td>top_5_accuracy</td><td>0.87966</td></tr><tr><td>top_10_accuracy</td><td>0.92018</td></tr><tr><td>_runtime</td><td>208</td></tr><tr><td>_timestamp</td><td>1660821167</td></tr><tr><td>_step</td><td>9</td></tr><tr><td>global_step</td><td>4400</td></tr><tr><td>train_loss</td><td>0.00417</td></tr><tr><td>Training loss</td><td>0.13136</td></tr><tr><td>lr</td><td>0.0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval_loss</td><td>▁▁</td></tr><tr><td>mrr@1</td><td>▁▁</td></tr><tr><td>mrr@2</td><td>▁▁</td></tr><tr><td>mrr@3</td><td>▁▁</td></tr><tr><td>mrr@5</td><td>▁▁</td></tr><tr><td>mrr@10</td><td>▁▁</td></tr><tr><td>top_1_accuracy</td><td>▁▁</td></tr><tr><td>top_2_accuracy</td><td>▁▁</td></tr><tr><td>top_3_accuracy</td><td>▁▁</td></tr><tr><td>top_5_accuracy</td><td>▁▁</td></tr><tr><td>top_10_accuracy</td><td>▁▁</td></tr><tr><td>_runtime</td><td>▁▁▂▃▄▅▅▆▇█</td></tr><tr><td>_timestamp</td><td>▁▁▂▃▄▅▅▆▇█</td></tr><tr><td>_step</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>global_step</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>Training loss</td><td>▂▁▆▁█▂▃▅</td></tr><tr><td>lr</td><td>█▇▆▅▄▃▂▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">devout-feather-25</strong>: <a href=\"https://wandb.ai/thilina/Training%20DPR%20Tests/runs/2iav62og\" target=\"_blank\">https://wandb.ai/thilina/Training%20DPR%20Tests/runs/2iav62og</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:2iav62og). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "2022-08-18 13:16:11.500419: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.32<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">ethereal-sun-26</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/thilina/Training%20DPR%20Tests\" target=\"_blank\">https://wandb.ai/thilina/Training%20DPR%20Tests</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/thilina/Training%20DPR%20Tests/runs/np59rvia\" target=\"_blank\">https://wandb.ai/thilina/Training%20DPR%20Tests/runs/np59rvia</a><br/>\n",
       "                Run data is saved locally in <code>/deep_learning/WandB Seminar/wandb/run-20220818_131606-np59rvia</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.retrieval.retrieval_model:{'eval_loss': 24.18571879646995, 'mrr@1': 0.6581734458940905, 'mrr@2': 0.7214121258633922, 'mrr@3': 0.7382962394474291, 'mrr@5': 0.7491864927091327, 'mrr@10': 0.7545236267953075, 'top_1_accuracy': 0.6581734458940905, 'top_2_accuracy': 0.7846508058326938, 'top_3_accuracy': 0.8353031465848043, 'top_5_accuracy': 0.8825786646201075, 'top_10_accuracy': 0.9215656178050652}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8903d55ac254e85b5e536a606e80227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 5:   0%|          | 0/1472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2728784/407699653.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/simple-transformers/simpletransformers/retrieval/retrieval_model.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, train_data, output_dir, show_running_loss, args, eval_data, additional_eval_passages, clustered_training, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         )\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/simple-transformers/simpletransformers/retrieval/retrieval_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, output_dir, show_running_loss, eval_data, additional_eval_passages, clustered_training, train_data, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m                     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/st/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/st/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train_model(train_data, eval_data=eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthilina\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "run_path = \"thilina/Dense retrieval with Simple Transformers/3k77hhta\"\n",
    "\n",
    "run = api.run(run_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training losses at each logging step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_step</th>\n",
       "      <th>_runtime</th>\n",
       "      <th>lr</th>\n",
       "      <th>Training loss</th>\n",
       "      <th>global_step</th>\n",
       "      <th>_timestamp</th>\n",
       "      <th>top_1_accuracy</th>\n",
       "      <th>mrr@1</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>top_3_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>gradients/graph_1embeddings.word_embeddings.weight</th>\n",
       "      <th>gradients/encoder.layer.6.attention.self.query.bias</th>\n",
       "      <th>gradients/encoder.layer.8.attention.output.dense.bias</th>\n",
       "      <th>gradients/graph_1encoder.layer.7.attention.self.query.bias</th>\n",
       "      <th>gradients/embeddings.word_embeddings.weight</th>\n",
       "      <th>gradients/encoder.layer.6.attention.self.query.weight</th>\n",
       "      <th>gradients/graph_1encoder.layer.11.attention.output.LayerNorm.bias</th>\n",
       "      <th>gradients/graph_1encoder.layer.6.output.LayerNorm.weight</th>\n",
       "      <th>gradients/graph_1encoder.layer.6.output.dense.bias</th>\n",
       "      <th>gradients/encoder.layer.4.attention.self.key.bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>174</td>\n",
       "      <td>9.906595e-07</td>\n",
       "      <td>9.326607e+00</td>\n",
       "      <td>350.0</td>\n",
       "      <td>1660863692</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>391</td>\n",
       "      <td>2.264365e-06</td>\n",
       "      <td>3.730740e+00</td>\n",
       "      <td>800.0</td>\n",
       "      <td>1660863909</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>415</td>\n",
       "      <td>2.405887e-06</td>\n",
       "      <td>3.454845e+00</td>\n",
       "      <td>850.0</td>\n",
       "      <td>1660863933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>439</td>\n",
       "      <td>2.547410e-06</td>\n",
       "      <td>2.934197e+00</td>\n",
       "      <td>900.0</td>\n",
       "      <td>1660863957</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>463</td>\n",
       "      <td>2.688933e-06</td>\n",
       "      <td>2.701782e+00</td>\n",
       "      <td>950.0</td>\n",
       "      <td>1660863981</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>1299</td>\n",
       "      <td>40719</td>\n",
       "      <td>1.228612e-07</td>\n",
       "      <td>4.877671e-03</td>\n",
       "      <td>58200.0</td>\n",
       "      <td>1660904237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>1302</td>\n",
       "      <td>40791</td>\n",
       "      <td>9.575948e-08</td>\n",
       "      <td>7.152554e-08</td>\n",
       "      <td>58350.0</td>\n",
       "      <td>1660904309</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>1303</td>\n",
       "      <td>40815</td>\n",
       "      <td>8.672557e-08</td>\n",
       "      <td>1.490113e-07</td>\n",
       "      <td>58400.0</td>\n",
       "      <td>1660904333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>1305</td>\n",
       "      <td>40863</td>\n",
       "      <td>6.865774e-08</td>\n",
       "      <td>9.651372e-06</td>\n",
       "      <td>58500.0</td>\n",
       "      <td>1660904381</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>1311</td>\n",
       "      <td>41009</td>\n",
       "      <td>1.445426e-08</td>\n",
       "      <td>1.072870e-06</td>\n",
       "      <td>58800.0</td>\n",
       "      <td>1660904527</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>453 rows × 416 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     _step  _runtime            lr  Training loss  global_step  _timestamp  \\\n",
       "0        6       174  9.906595e-07   9.326607e+00        350.0  1660863692   \n",
       "1       15       391  2.264365e-06   3.730740e+00        800.0  1660863909   \n",
       "2       16       415  2.405887e-06   3.454845e+00        850.0  1660863933   \n",
       "3       17       439  2.547410e-06   2.934197e+00        900.0  1660863957   \n",
       "4       18       463  2.688933e-06   2.701782e+00        950.0  1660863981   \n",
       "..     ...       ...           ...            ...          ...         ...   \n",
       "509   1299     40719  1.228612e-07   4.877671e-03      58200.0  1660904237   \n",
       "510   1302     40791  9.575948e-08   7.152554e-08      58350.0  1660904309   \n",
       "511   1303     40815  8.672557e-08   1.490113e-07      58400.0  1660904333   \n",
       "512   1305     40863  6.865774e-08   9.651372e-06      58500.0  1660904381   \n",
       "513   1311     41009  1.445426e-08   1.072870e-06      58800.0  1660904527   \n",
       "\n",
       "     top_1_accuracy  mrr@1  train_loss  top_3_accuracy  ...  \\\n",
       "0               NaN    NaN         NaN             NaN  ...   \n",
       "1               NaN    NaN         NaN             NaN  ...   \n",
       "2               NaN    NaN         NaN             NaN  ...   \n",
       "3               NaN    NaN         NaN             NaN  ...   \n",
       "4               NaN    NaN         NaN             NaN  ...   \n",
       "..              ...    ...         ...             ...  ...   \n",
       "509             NaN    NaN         NaN             NaN  ...   \n",
       "510             NaN    NaN         NaN             NaN  ...   \n",
       "511             NaN    NaN         NaN             NaN  ...   \n",
       "512             NaN    NaN         NaN             NaN  ...   \n",
       "513             NaN    NaN         NaN             NaN  ...   \n",
       "\n",
       "     gradients/graph_1embeddings.word_embeddings.weight  \\\n",
       "0                                                  NaN    \n",
       "1                                                  NaN    \n",
       "2                                                  NaN    \n",
       "3                                                  NaN    \n",
       "4                                                  NaN    \n",
       "..                                                 ...    \n",
       "509                                                NaN    \n",
       "510                                                NaN    \n",
       "511                                                NaN    \n",
       "512                                                NaN    \n",
       "513                                                NaN    \n",
       "\n",
       "     gradients/encoder.layer.6.attention.self.query.bias  \\\n",
       "0                                                  NaN     \n",
       "1                                                  NaN     \n",
       "2                                                  NaN     \n",
       "3                                                  NaN     \n",
       "4                                                  NaN     \n",
       "..                                                 ...     \n",
       "509                                                NaN     \n",
       "510                                                NaN     \n",
       "511                                                NaN     \n",
       "512                                                NaN     \n",
       "513                                                NaN     \n",
       "\n",
       "     gradients/encoder.layer.8.attention.output.dense.bias  \\\n",
       "0                                                  NaN       \n",
       "1                                                  NaN       \n",
       "2                                                  NaN       \n",
       "3                                                  NaN       \n",
       "4                                                  NaN       \n",
       "..                                                 ...       \n",
       "509                                                NaN       \n",
       "510                                                NaN       \n",
       "511                                                NaN       \n",
       "512                                                NaN       \n",
       "513                                                NaN       \n",
       "\n",
       "     gradients/graph_1encoder.layer.7.attention.self.query.bias  \\\n",
       "0                                                  NaN            \n",
       "1                                                  NaN            \n",
       "2                                                  NaN            \n",
       "3                                                  NaN            \n",
       "4                                                  NaN            \n",
       "..                                                 ...            \n",
       "509                                                NaN            \n",
       "510                                                NaN            \n",
       "511                                                NaN            \n",
       "512                                                NaN            \n",
       "513                                                NaN            \n",
       "\n",
       "     gradients/embeddings.word_embeddings.weight  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "..                                           ...   \n",
       "509                                          NaN   \n",
       "510                                          NaN   \n",
       "511                                          NaN   \n",
       "512                                          NaN   \n",
       "513                                          NaN   \n",
       "\n",
       "     gradients/encoder.layer.6.attention.self.query.weight  \\\n",
       "0                                                  NaN       \n",
       "1                                                  NaN       \n",
       "2                                                  NaN       \n",
       "3                                                  NaN       \n",
       "4                                                  NaN       \n",
       "..                                                 ...       \n",
       "509                                                NaN       \n",
       "510                                                NaN       \n",
       "511                                                NaN       \n",
       "512                                                NaN       \n",
       "513                                                NaN       \n",
       "\n",
       "     gradients/graph_1encoder.layer.11.attention.output.LayerNorm.bias  \\\n",
       "0                                                  NaN                   \n",
       "1                                                  NaN                   \n",
       "2                                                  NaN                   \n",
       "3                                                  NaN                   \n",
       "4                                                  NaN                   \n",
       "..                                                 ...                   \n",
       "509                                                NaN                   \n",
       "510                                                NaN                   \n",
       "511                                                NaN                   \n",
       "512                                                NaN                   \n",
       "513                                                NaN                   \n",
       "\n",
       "     gradients/graph_1encoder.layer.6.output.LayerNorm.weight  \\\n",
       "0                                                  NaN          \n",
       "1                                                  NaN          \n",
       "2                                                  NaN          \n",
       "3                                                  NaN          \n",
       "4                                                  NaN          \n",
       "..                                                 ...          \n",
       "509                                                NaN          \n",
       "510                                                NaN          \n",
       "511                                                NaN          \n",
       "512                                                NaN          \n",
       "513                                                NaN          \n",
       "\n",
       "    gradients/graph_1encoder.layer.6.output.dense.bias  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "509                                                NaN   \n",
       "510                                                NaN   \n",
       "511                                                NaN   \n",
       "512                                                NaN   \n",
       "513                                                NaN   \n",
       "\n",
       "    gradients/encoder.layer.4.attention.self.key.bias  \n",
       "0                                                 NaN  \n",
       "1                                                 NaN  \n",
       "2                                                 NaN  \n",
       "3                                                 NaN  \n",
       "4                                                 NaN  \n",
       "..                                                ...  \n",
       "509                                               NaN  \n",
       "510                                               NaN  \n",
       "511                                               NaN  \n",
       "512                                               NaN  \n",
       "513                                               NaN  \n",
       "\n",
       "[453 rows x 416 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.history().dropna(subset=[\"Training loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.output_scroll { height: 100em; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_scroll { height: 100em; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://wandb.ai/thilina/Dense%20retrieval%20with%20Simple%20Transformers/workspace?jupyter=true\" style=\"border:none;width:100%;height:1024px;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x7f10c9931490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%wandb thilina/Dense%20retrieval%20with%20Simple%20Transformers -h 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c82b770e094df22bcf50b008e2cbf605d7a6641caec9ae0ba18a77a161e4d1a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
